---
title: "Distributional Regression and Quantile Regression Examples"
output:
  html_document:
    df_print: paged
---

In this document I am trying to implement
distributional models using BRMS. I am 
following an [example](https://www.tmwolock.com/index.php/2020/12/18/distributional-regression-models-brms-tutorial/) from Tim Wolock, accompanying paper [here](https://elifesciences.org/articles/68318.pdf). Other useful resources are this [vignette](https://cran.r-project.org/web/packages/brms/vignettes/brms_distreg.html) from Paul Burkner, and the accompanying [paper](https://arxiv.org/pdf/1705.11123.pdf)

The procedure will be:

* Generate data from a particular model
* Compare range of models on the data (e.g. normal with varying variance, skew-normal, quantile, GAMLSS)

Generate more and more complex data, then compare again

## Setup  

```{r, warning=F, message=F}
library(brms) 
library(quantreg)
library(ggplot2)
library(magrittr)
library(tidyr)

library(parallel)

n_cores <- detectCores(all.tests = FALSE, logical = TRUE)
set.seed(404)
```

## Heteroskedastic Normal Model

We begin with a model where Y is allow to vary as
a function of X, and the variance is also
allowed to vary as a function of X

### Generating data


$$
y_i\;{\sim}\;\mathcal{N}(\mu_i, \sigma_i)
$$
$$
\mu_i=\beta^{\mu}X^{\mu}_i
$$
$$
\log(\sigma_i)=\beta^{\sigma}X^{\sigma}_i
$$

In words these equations mean that $y_i$, our target variable is, is normally distributed with mean $\mu_i$
and variance $\sigma_i$. $\mu_i$ can be modelled as a linear combination of predictor
variables $X_i^\mu$, with coefficients $\beta^\mu$. Finally $\sigma_i$ can be modelled as
the exponential linear combination of predictors $X^{\sigma}_i$, and coefficients $\beta^{\sigma}$
($e^{\beta^{\sigma}X^{\sigma}_i}$).

```{r}
N <- 200
X <- cbind(1, runif(N, 0, 100)) # Generate N uniform random numbers between 0-100

B_mu <- c(10, 0.2)
B_sigma <- c(0, 0.022)

mu <- X %*% B_mu # Matrix Multiplication
sigma <- exp(X %*% B_sigma)

y <- rnorm(N, mu, sigma)
sim_data <- data.frame(y, X)


```


```{r, warning=F, message=F}
ggplot() +
  geom_point(aes(x = X[,2], y = y)) 
```


### Simple Linear Regression

Now we try to fit a model it using a simple linear
regression (where the variance remains constant). 
For the simple linear regression 
model we have:

$$
y_i\;{\sim}\;\mathcal{N}(\mu_i, \sigma)
$$
$$
\mu_i=\beta^{\mu}X^{\mu}_i
$$

$$
\sigma=const
$$


```{r, warning=FALSE,message=FALSE,  results='hide'}

# Set up brms model
conv_fm <- bf(
  y ~ X2
)
conv_brm <- brm(conv_fm, data = sim_data,cores = n_cores-1)

```
```{r}
print(conv_brm)
```



#### Predictive checks

We check what the model looks like against the data. 

```{r, message=FALSE, warning=FALSE}
pred_df <- data.frame(X2 = seq(0, 100))

conv_post_pred <- cbind(pred_df, predict(conv_brm, newdata = pred_df))
head(conv_post_pred)


ggplot() +
  geom_point(data=sim_data, aes(y=y, x=X2, shape="Data Point"))+
  scale_shape_manual("",values=c(16))+
  
  geom_ribbon(data=conv_post_pred,aes(x=X2, ymin=Q2.5, ymax=Q97.5,fill = "Confidence Interval"), alpha=0.5)+
  scale_fill_manual("",values="grey12") +
  
  geom_line(data=conv_post_pred,aes(x=X2, y=Estimate, color="Estimate"))+
  scale_colour_manual(c("",""),values=c("black","black"))

```

As we can see here, the model is not capturing the fact that
the variance increases.

### Distribitional Model

Now we fit the distributional mode, allowing the 
variance to change as a function of X.

```{r, message=FALSE, warning=FALSE, results='hide'}
dist_fm <- bf(
  y ~ X2,
  sigma ~ X2
)
dist_brm <- brm(dist_fm, data = sim_data,cores = n_cores-1)
```

```{r}
print(dist_brm)
```

#### Predictive Checks

```{r, message=FALSE, warning=FALSE}

dist_post_pred <- cbind(pred_df, predict(dist_brm, newdata = pred_df))
# head(dist_post_pred)


ggplot() +
  geom_point(data=sim_data, aes(y=y, x=X2, shape="Data Point"))+
  scale_shape_manual("",values=c(16))+
  
  geom_ribbon(data=dist_post_pred,aes(x=X2, ymin=Q2.5, ymax=Q97.5,fill = "Confidence Interval"), alpha=0.5)+
  scale_fill_manual("",values="grey12") +
  
  geom_line(data=dist_post_pred,aes(x=X2, y=Estimate, color="Estimate"))+
  scale_colour_manual(c("",""),values=c("black","black"))


```


This time we see that the variance does indeed change as a function of X.

### Model Comparison (Against TRUE)

Lets see how the simple regression and 
the distributional model perform against one another.

```{r, message=F, warning=F, echo=F}

conv_fixef <- data.frame(fixef(conv_brm))
conv_fixef$variable <- rownames(fixef(conv_brm))
tmp_sigma <- data.frame(log(t(summary(conv_brm)$spec_pars[1:4])),variable = 'sigma_Intercept')
tmp_sigma$names <- colnames(conv_fixef)[c(1:4)] 
tmp_sigma <- tmp_sigma %>% 
  tidyr::pivot_wider(
                     id_cols = variable,
                     names_from = names, 
                     values_from = sigma,
                     id_expand = F)
conv_fixef <- rbind(conv_fixef, tmp_sigma)
conv_fixef$Model <- 'Conventional'
 
dist_fixef <- data.frame(fixef(dist_brm))
dist_fixef$variable <- rownames(fixef(dist_brm))
dist_fixef$Model <- 'Distributional'
 
true_fixef <- data.frame(
  variable = c('mu_Intercept', 'mu_X2', 'sigma_Intercept', 'sigma_X2'),
  value = c(B_mu, B_sigma)
)
 
all_fixef <- rbind(conv_fixef, dist_fixef)
 
all_fixef$variable[all_fixef$variable == 'Intercept'] <- 'mu_Intercept'
all_fixef$variable[all_fixef$variable == 'X2'] <- 'mu_X2'
ggplot() +
  geom_crossbar(data = all_fixef, aes(x=Model, y = Estimate, ymin = Q2.5, ymax=Q97.5),
                width=0.2, fill='darkcyan', color='darkcyan', alpha=0.7) +
  geom_hline(data = true_fixef, aes(yintercept=value)) +
  facet_wrap('variable', scales='free_y') +
  labs(y = NULL)
```

The horizontal black line indicates the true values (the 
parameters used to generate these data). We see that
the distributional model outperforms (as we expect).



<!-- ## Model Comparison (LOO) -->


<!-- ```{r, warning=F, message=F} -->
<!-- loo_res <- brms::loo(conv_brm, dist_brm) -->
<!-- loo_comparison <- loo_res$diffs -->

<!-- print(loo_res) -->
<!-- ``` -->

## Skewed Normal Distribution

This time we will go through the same process, except our data
will be generated using a model allows location, scale and shape to
vary (i.e. we are adding skewness). We generate
data from this equations:

$$
y_i\;{\sim}\;\mathcal{f}(\mu_i, \sigma_i, \nu_i)
$$
Where $\mu_i$ is the location parameter (e.g. mean), $\sigma_i$ is the 
scale parameter (e.g. standard deviation), and $\nu_i$ is the shape
parameter ($\nu_i$ generally representing skew, sometimes an extra shape 
parameter $\tau_i$ is modelled,representing kurtosis). Here are the 
equations we are using to model our data:

$$
\mu_i=\beta^{\mu}X^{\mu}_i
$$

$$
\log(\sigma_i)=\beta^{\sigma}X^{\sigma}_i
$$

$$
\nu_i=\beta^{\nu}X^{\nu}_i
$$


Here we generate the data:


```{r, message=FALSE, warning=FALSE}
N <- 200
X <- cbind(1, runif(N, 0, 100)) # Generate N uniform random numbers between 0-100

# Setting Distributions Params for Coefficients
B_mu <- c(10, 0.2) 
B_sigma <- c(0.1, 0.02)
# Note it is nu for gamlss.dist, but alpha for brms
B_nu <- c(2, 0.5)

# Generating Coefficients for X values
mu <- X %*% B_mu 
sigma <- exp(X %*% B_sigma)
nu <- exp(X %*% B_nu)

# Generating y-values
y <- brms::rskew_normal(n=N, mu = mu, sigma = sigma, alpha = nu)
sim_data <- data.frame(y, X)
```

```{r, warning=F, message=F}
ggplot() +
  geom_point(aes(x = X[,2], y = y)) 

```


### Simple Regression

First we fit our conventional regression model

```{r, warning=FALSE,message=FALSE,  results='hide'}

conv_fm <- bf(
  y ~ X2
)
conv_brm <- brm(conv_fm, data = sim_data,cores = n_cores-1)
```

```{r, warning=F, message=F}
pred_df <- data.frame(X2 = seq(0, 100))

conv_post_pred <- cbind(pred_df, predict(conv_brm, newdata = pred_df))


ggplot() +
  geom_point(data=sim_data, aes(y=y, x=X2, shape="Data Point"))+
  scale_shape_manual("",values=c(16))+
  
  geom_ribbon(data=conv_post_pred,aes(x=X2, ymin=Q2.5, ymax=Q97.5,fill = "Confidence Interval"), alpha=0.5)+
  scale_fill_manual("",values="grey12") +
  
  geom_line(data=conv_post_pred,aes(x=X2, y=Estimate, color="Estimate"))+
  scale_colour_manual(c("",""),values=c("black","black"))
```


Again, we see that changes in variance and skew
are not accounted for by the simple regression model.

### Distribitional Model (changing variance, zero skew)

Now we build a model, allowing variance 
to change, but keeping skew constant (zero)

```{r, message=FALSE, warning=FALSE, results='hide'}
dist_fm <- bf(
  y ~ X2,
  sigma ~ X2
)
dist_brm <- brm(dist_fm, data = sim_data,cores = n_cores-1)
```

```{r}
print(dist_brm)
```

#### Predictive Checks

We check the model against
the original dataset.


```{r, message=FALSE, warning=FALSE}

dist_post_pred <- cbind(pred_df, predict(dist_brm, newdata = pred_df))
# head(dist_post_pred)


ggplot() +
  geom_point(data=sim_data, aes(y=y, x=X2, shape="Data Point"))+
  scale_shape_manual("",values=c(16))+
  
  geom_ribbon(data=dist_post_pred,aes(x=X2, ymin=Q2.5, ymax=Q97.5,fill = "Confidence Interval"), alpha=0.5)+
  scale_fill_manual("",values="grey12") +
  
  geom_line(data=dist_post_pred,aes(x=X2, y=Estimate, color="Estimate"))+
  scale_colour_manual(c("",""),values=c("black","black"))
```


Better than our previous model, but not capturing the
skew in the distribution.

### Location, Scale and Shape Model

Now we build a final model, accounting for
differences in location, scale, and shape.

```{r, message=FALSE, warning=FALSE, results='hide'}

# Maybe take independent priors?

lss_fm <- bf(
  y ~ X2,
  sigma ~ X2,
  alpha ~ X2
)

lss_brm <- brm(lss_fm, 
               data = sim_data, 
               family = skew_normal(link = 'identity', link_sigma = 'log', link_alpha = 'log'),
               control=list(adapt_delta = 0.9999),
              cores = n_cores-1)

```

```{r}
print(lss_brm)
```

#### Predictive Checks

```{r, message=FALSE, warning=FALSE}

lss_post_pred <- cbind(pred_df, predict(lss_brm, newdata = pred_df))
# head(lss_post_pred)


ggplot() +
  geom_point(data=sim_data, aes(y=y, x=X2, shape="Data Point"))+
  scale_shape_manual("",values=c(16))+
  
  geom_ribbon(data=lss_post_pred,aes(x=X2, ymin=Q2.5, ymax=Q97.5,fill = "Confidence Interval"), alpha=0.5)+
  scale_fill_manual("",values="grey12") +
  
  geom_line(data=lss_post_pred,aes(x=X2, y=Estimate, color="Estimate"))+
  scale_colour_manual(c("",""),values=c("black","black"))


```



<!-- ## Quantile -->


## Other Useful Resources

* [Tidy Bayes](http://mjskay.github.io/tidybayes/articles/tidy-brms.html)
* [brms families](https://cran.r-project.org/web/packages/brms/vignettes/brms_families.html)
* Doing bayesian analysis in BRMS, see [here](https://bookdown.org/content/3686/)
* What to do for [divergence](https://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup)


## Notes

* Student-T distribution uses a "centile-based" measure of location?
* Can compare the power of quantile, 